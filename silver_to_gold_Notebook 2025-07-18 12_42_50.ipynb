{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "732ed771-6d93-480c-bce4-a60782aad36f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "service_credential = dbutils.secrets.get(scope=\"blockpulse_keyvault_scope3\",key=\"blockpulsedatabrickssecret\")\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.blockpulsecrypto.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.blockpulsecrypto.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.blockpulsecrypto.dfs.core.windows.net\", \"39057514-1421-4818-9f98-8bcc69c384d4\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.blockpulsecrypto.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.blockpulsecrypto.dfs.core.windows.net\", \"https://login.microsoftonline.com/0dba45aa-3354-4d4c-8c45-eea7b490aebe/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb04e391-c30d-40a7-8f4b-9af3e713da77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table already exists at abfss://blockpulsedatacontainer@blockpulsecrypto.dfs.core.windows.net/gold/crypto_market_snapshot_fact\nDelta table already exists at abfss://blockpulsedatacontainer@blockpulsecrypto.dfs.core.windows.net/gold/crypto_asset_dim\nDelta table already exists at abfss://blockpulsedatacontainer@blockpulsecrypto.dfs.core.windows.net/gold/date_dim\ncrypto_market_snapshot_fact successfully written to gold path.\ncrypto_asset_dim successfully written to gold path.\ndate_dim successfully written to gold path.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, current_timestamp, lit, coalesce, year, month, dayofmonth, dayofweek\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, DateType,\n",
    "    DoubleType, LongType\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Blockpulse Gold Layer\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "storage_account = \"blockpulsecrypto\"\n",
    "container = \"blockpulsedatacontainer\"\n",
    "today = datetime.now()\n",
    "current_year, current_month, current_day = today.year, today.month, today.day\n",
    "\n",
    "silver_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/silver/year={current_year}/month={current_month:02d}/day={current_day:02d}/\"\n",
    "gold_path = f\"abfss://{container}@{storage_account}.dfs.core.windows.net/gold/\"\n",
    "\n",
    "# Define Gold Table Schemas\n",
    "gold_tables = {\n",
    "    \"crypto_market_snapshot_fact\": StructType([\n",
    "        StructField(\"crypto_id\", StringType()),\n",
    "        StructField(\"snapshot_date\", DateType()),\n",
    "        StructField(\"current_price_usd\", DoubleType()),\n",
    "        StructField(\"market_cap_usd\", DoubleType()),\n",
    "        StructField(\"total_volume_usd\", DoubleType()),\n",
    "        StructField(\"volume_change_pct_24h\", DoubleType()),\n",
    "        StructField(\"price_change_24h_usd\", DoubleType()),\n",
    "        StructField(\"price_change_pct_24h\", DoubleType()),\n",
    "        StructField(\"market_cap_rank\", LongType()),\n",
    "        StructField(\"circulating_supply\", DoubleType()),\n",
    "        StructField(\"total_supply\", DoubleType()),\n",
    "        StructField(\"max_supply\", DoubleType()),\n",
    "        StructField(\"all_time_high\", DoubleType()),\n",
    "        StructField(\"ath_change_pct\", DoubleType()),\n",
    "        StructField(\"all_time_low\", DoubleType()),\n",
    "        StructField(\"atl_change_pct\", DoubleType()),\n",
    "        StructField(\"high_24h_usd\", DoubleType()),\n",
    "        StructField(\"low_24h_usd\", DoubleType())\n",
    "    ]),\n",
    "    \"crypto_asset_dim\": StructType([\n",
    "        StructField(\"crypto_id\", StringType()),\n",
    "        StructField(\"symbol\", StringType()),\n",
    "        StructField(\"name\", StringType()),\n",
    "        StructField(\"image_url\", StringType())\n",
    "    ]),\n",
    "    \"date_dim\": StructType([\n",
    "        StructField(\"date\", DateType()),\n",
    "        StructField(\"year\", LongType()),\n",
    "        StructField(\"month\", LongType()),\n",
    "        StructField(\"day\", LongType()),\n",
    "        StructField(\"day_of_week\", LongType())\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Ensure Gold Tables Exist\n",
    "def ensure_delta_table(path, schema=None):\n",
    "    try:\n",
    "        if not DeltaTable.isDeltaTable(spark, path):\n",
    "            print(f\"Creating Delta table at {path}\")\n",
    "            empty_df = spark.createDataFrame([], schema or StructType([]))\n",
    "            empty_df.write.format(\"delta\").mode(\"overwrite\").save(path)\n",
    "        else:\n",
    "            print(f\"Delta table already exists at {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error ensuring table at {path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "for table_name, schema in gold_tables.items():\n",
    "    ensure_delta_table(f\"{gold_path}{table_name}\", schema)\n",
    "\n",
    "# Read Silver Tables\n",
    "try:\n",
    "    crypto_market_snapshot_fact_df = spark.read.format(\"delta\").load(f\"{silver_path}crypto_market_snapshot_fact/\")\n",
    "    crypto_asset_dim_df = spark.read.format(\"delta\").load(f\"{silver_path}crypto_asset_dim/\")\n",
    "    date_dim_df = spark.read.format(\"delta\").load(f\"{silver_path}date_dim/\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from silver path: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Add processing metadata (optional)\n",
    "for df in [crypto_market_snapshot_fact_df, crypto_asset_dim_df, date_dim_df]:\n",
    "    df.withColumn(\"processing_time\", current_timestamp())\n",
    "\n",
    "# Write Gold Tables\n",
    "def write_to_gold(df, table_name):\n",
    "    try:\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}{table_name}\")\n",
    "        print(f\"{table_name} successfully written to gold path.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing {table_name} to gold layer:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "write_to_gold(crypto_market_snapshot_fact_df, \"crypto_market_snapshot_fact\")\n",
    "write_to_gold(crypto_asset_dim_df, \"crypto_asset_dim\")\n",
    "write_to_gold(date_dim_df, \"date_dim\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_to_gold_Notebook 2025-07-18 12:42:50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}